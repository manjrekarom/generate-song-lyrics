{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirito/anaconda3/envs/tensorflow-env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>from</th>\n",
       "      <th>href</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aa Jao Na</td>\n",
       "      <td>[from \"Veere Di Wedding\" soundtrack]</td>\n",
       "      <td>/lyrics/bollywood/aajaona.html</td>\n",
       "      <td>\\n\\r\\nTum thhe yahin\\nPhir bhi tum gum thhe\\nA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aaj Phir</td>\n",
       "      <td>[from \"Hate Story 2\" soundtrack]</td>\n",
       "      <td>/lyrics/arijitsingh/aajphir.html</td>\n",
       "      <td>\\n\\r\\nAaj phir tumpe pyar aaya hai\\nAaj phir t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aaj Se Teri</td>\n",
       "      <td>[from \"Padman\" soundtrack]</td>\n",
       "      <td>/lyrics/arijitsingh/aajseteri.html</td>\n",
       "      <td>\\n\\r\\nAaj se teri saari galiyan meri ho gayi\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aasan Nahin Yahan</td>\n",
       "      <td>[from \"Aashiqui 2\" soundtrack]</td>\n",
       "      <td>/lyrics/arijitsingh/aasannahinyahan.html</td>\n",
       "      <td>\\n\\r\\nWo o o o...\\n\\nAasaan nahi yahaan aashiq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ab Raat</td>\n",
       "      <td>[from \"Dobaara\" soundtrack]</td>\n",
       "      <td>/lyrics/arijitsingh/abraat.html</td>\n",
       "      <td>\\n\\r\\nChaand ki aankhein bhaari si hain\\nRaat ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name                                  from  \\\n",
       "0          Aa Jao Na  [from \"Veere Di Wedding\" soundtrack]   \n",
       "1           Aaj Phir      [from \"Hate Story 2\" soundtrack]   \n",
       "2        Aaj Se Teri            [from \"Padman\" soundtrack]   \n",
       "3  Aasan Nahin Yahan        [from \"Aashiqui 2\" soundtrack]   \n",
       "4            Ab Raat           [from \"Dobaara\" soundtrack]   \n",
       "\n",
       "                                       href  \\\n",
       "0            /lyrics/bollywood/aajaona.html   \n",
       "1          /lyrics/arijitsingh/aajphir.html   \n",
       "2        /lyrics/arijitsingh/aajseteri.html   \n",
       "3  /lyrics/arijitsingh/aasannahinyahan.html   \n",
       "4           /lyrics/arijitsingh/abraat.html   \n",
       "\n",
       "                                              lyrics  \n",
       "0  \\n\\r\\nTum thhe yahin\\nPhir bhi tum gum thhe\\nA...  \n",
       "1  \\n\\r\\nAaj phir tumpe pyar aaya hai\\nAaj phir t...  \n",
       "2  \\n\\r\\nAaj se teri saari galiyan meri ho gayi\\n...  \n",
       "3  \\n\\r\\nWo o o o...\\n\\nAasaan nahi yahaan aashiq...  \n",
       "4  \\n\\r\\nChaand ki aankhein bhaari si hain\\nRaat ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/arijit_songs_with_lyrics.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Lyrics \n",
    "1. Get all the lyrics into one single string(for simplicity).\n",
    "2. Divide the string into characters. \n",
    "3. Group characters of size given by timesteps together for input.\n",
    "4. Output will be the character just following the input.\n",
    "5. Slide the window of size timestep by number of steps(=1) to make other inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get all the lyrics into one single string(for simplicity)\n",
    "text = '' # will hold all the text data\n",
    "for idx, row in df['lyrics'].iteritems():\n",
    "    text = text + row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n', '\\r', '2', '[', ']', 'a', 'b', 'c', 'd', 'x']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# playground code - can be removed\n",
    "str('abc')\n",
    "sorted(list(set('abc\\rd[ax2]b\\n')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert text to lower case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Divide the string into characters. \n",
    "list_chars = sorted(list(set(lower_text)))\n",
    "len(list_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'\\n': 6493,\n",
       "         '\\r': 129,\n",
       "         't': 4250,\n",
       "         'u': 3617,\n",
       "         'm': 3860,\n",
       "         ' ': 19854,\n",
       "         'h': 9034,\n",
       "         'e': 8915,\n",
       "         'y': 2584,\n",
       "         'a': 21016,\n",
       "         'i': 8612,\n",
       "         'n': 6862,\n",
       "         'p': 1359,\n",
       "         'r': 5118,\n",
       "         'b': 2151,\n",
       "         'g': 1448,\n",
       "         'l': 2593,\n",
       "         'j': 2024,\n",
       "         'o': 4723,\n",
       "         's': 3392,\n",
       "         '[': 141,\n",
       "         'x': 140,\n",
       "         '2': 108,\n",
       "         ']': 141,\n",
       "         'd': 3055,\n",
       "         'w': 625,\n",
       "         'k': 3936,\n",
       "         'c': 1000,\n",
       "         '?': 24,\n",
       "         '…': 340,\n",
       "         'v': 294,\n",
       "         'z': 574,\n",
       "         'q': 211,\n",
       "         '-': 164,\n",
       "         'f': 311,\n",
       "         ',': 487,\n",
       "         '.': 1130,\n",
       "         '(': 65,\n",
       "         ')': 65,\n",
       "         '!': 49,\n",
       "         \"'\": 38,\n",
       "         '4': 16,\n",
       "         '3': 7,\n",
       "         '’': 9,\n",
       "         '8': 3,\n",
       "         ':': 7,\n",
       "         'é': 10,\n",
       "         '*': 1,\n",
       "         '‘': 3,\n",
       "         '1': 6,\n",
       "         '6': 2,\n",
       "         '0': 6,\n",
       "         '“': 2,\n",
       "         '”': 2,\n",
       "         '–': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "chars = collections.Counter(lower_text)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Divide the strings into size timesteps size\n",
    "# 4.\n",
    "# 5.\n",
    "timesteps = 20\n",
    "sentences = []\n",
    "outputs = []\n",
    "for i in range(0, len(lower_text)-timesteps):\n",
    "    sentences.append(lower_text[i: i + timesteps])\n",
    "    outputs.append(lower_text[i + timesteps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n\\r\\ntum thhe yahin\\nph', '\\r\\ntum thhe yahin\\nphi']\n",
      "130987\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0:2])\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'r']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Tokenize array of strings to chars \n",
    "tokenized_sentences = [list(sentence) for sentence in sentences]\n",
    "tokenized_outputs = [list(output) for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Create character-to-index and index-to-character mappings\n",
    "char_index = {char: idx for idx, char in enumerate(list_chars)}\n",
    "index_char = [char for char in list_chars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character to Index: -  {'\\n': 0, '\\r': 1, ' ': 2, '!': 3, \"'\": 4, '(': 5, ')': 6, '*': 7, ',': 8, '-': 9, '.': 10, '0': 11, '1': 12, '2': 13, '3': 14, '4': 15, '6': 16, '8': 17, ':': 18, '?': 19, '[': 20, ']': 21, 'a': 22, 'b': 23, 'c': 24, 'd': 25, 'e': 26, 'f': 27, 'g': 28, 'h': 29, 'i': 30, 'j': 31, 'k': 32, 'l': 33, 'm': 34, 'n': 35, 'o': 36, 'p': 37, 'q': 38, 'r': 39, 's': 40, 't': 41, 'u': 42, 'v': 43, 'w': 44, 'x': 45, 'y': 46, 'z': 47, 'é': 48, '–': 49, '‘': 50, '’': 51, '“': 52, '”': 53, '…': 54}\n",
      "Index to Character: -  ['\\n', '\\r', ' ', '!', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '6', '8', ':', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'é', '–', '‘', '’', '“', '”', '…']\n"
     ]
    }
   ],
   "source": [
    "print('Character to Index: - ', char_index)\n",
    "print('Index to Character: - ', index_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Label encoding\n",
    "le_tokenized_sentences = [[char_index[char] for char in sent] for sent in tokenized_sentences]\n",
    "le_tokenized_outputs = [[char_index[char] for char in output] for output in tokenized_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Convert to one hot encodings\n",
    "for i, sent in enumerate(le_tokenized_sentences):\n",
    "    for j, index in enumerate(sent):\n",
    "        le_tokenized_sentences[i][j] = np.zeros(len(list_chars))\n",
    "        le_tokenized_sentences[i][j][index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]),\n",
       " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]),\n",
       " array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]),\n",
       " array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]),\n",
       " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_tokenized_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, output in enumerate(le_tokenized_outputs):\n",
    "    for j, index in enumerate(output):\n",
    "        le_tokenized_outputs[i][j] = np.zeros(len(list_chars))\n",
    "        le_tokenized_outputs[i][j][index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_tokenized_outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "We will create a model to train it with the above sentences. One hot encoding of the sentences will be done here itself.\n",
    "1. Timesteps is 20\n",
    "2. Step is 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(timesteps, len(list_chars))))\n",
    "\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model and pick the loss and optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_tokenized_outputs = np.asanyarray(le_tokenized_outputs)\n",
    "le_tokenized_sentences = np.asanyarray(le_tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_tokenized_outputs = le_tokenized_outputs.reshape((-1, 55))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130987, 55)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_tokenized_sentences.shape\n",
    "le_tokenized_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "130987/130987 [==============================] - 54s 413us/step - loss: 2.0372\n",
      "Epoch 2/10\n",
      "130987/130987 [==============================] - 30s 232us/step - loss: 1.6313\n",
      "Epoch 3/10\n",
      "130987/130987 [==============================] - 43s 332us/step - loss: 1.4716\n",
      "Epoch 4/10\n",
      "130987/130987 [==============================] - 45s 347us/step - loss: 1.3828\n",
      "Epoch 5/10\n",
      "130987/130987 [==============================] - 43s 327us/step - loss: 1.3210\n",
      "Epoch 6/10\n",
      "130987/130987 [==============================] - 50s 383us/step - loss: 1.2756\n",
      "Epoch 7/10\n",
      "130987/130987 [==============================] - 44s 334us/step - loss: 1.2410\n",
      "Epoch 8/10\n",
      "130987/130987 [==============================] - 43s 325us/step - loss: 1.2139\n",
      "Epoch 9/10\n",
      "130987/130987 [==============================] - 47s 361us/step - loss: 1.1870\n",
      "Epoch 10/10\n",
      "130987/130987 [==============================] - 50s 382us/step - loss: 1.1693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f442320a940>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(le_tokenized_sentences, le_tokenized_outputs, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               94208     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 55)                7095      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 55)                0         \n",
      "=================================================================\n",
      "Total params: 101,303\n",
      "Trainable params: 101,303\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
